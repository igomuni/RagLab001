# 実験003: MoE Ollama + Phi3 + Nomic v1.5

**日付**: 未実施
**ステータス**: 🚧 準備完了（実行待ち）

## 設定

- **LLM**: Ollama phi3 (2.2GB)
- **埋め込み**: nomic-embed-text v1.5 (768次元、MoEアーキテクチャ、Ollama経由)
- **チャンク**: 500文字、50オーバーラップ
- **データ**: 日本語桃太郎テキスト3件 (21チャンク)

## 目的

MoE (Mixture of Experts) アーキテクチャを持つnomic-embed-text v1.5の効果を検証し、exp_001 (nomic-v1) と比較する。

## 主要な調査項目

1. **埋め込み品質**: MoEアーキテクチャによる検索精度の向上
2. **計算コスト**: 埋め込み生成時間の比較 (v1 vs v1.5)
3. **RAG応答品質**: 検索品質向上が応答品質に与える影響
4. **チャンク類似度**: 同じクエリに対して異なるチャンクが取得されるか

## 比較対象

- **exp_001**: 同じLLMだが nomic-v1 埋め込み（ベースライン比較）
- **exp_004**: 同じ埋め込みだが LM Studio + Phi-3.5（LLMの影響を分離）

## ファイル

- **ノートブック**: [notebook.ipynb](notebook.ipynb)
- **設定**: [config.json](config.json)
- **ベクトル**: vectors/ (Git除外)
- **結果**: results/ (Git除外)

## 実行前の準備

1. Ollamaが起動していることを確認
2. 必要なモデルをプル:
   ```bash
   ollama pull phi3
   ollama pull nomic-embed-text:latest  # v1.5を含む
   ```
3. ノートブックを開いて全セルを実行

## 予想される結果

### 仮説

1. **検索精度向上**: MoEアーキテクチャにより、より関連性の高いチャンクが取得される
2. **応答品質の微妙な改善**: 検索品質の向上がLLM応答の正確性を若干改善
3. **計算時間の増加**: MoEモデルは複雑なため、埋め込み生成に時間がかかる可能性

### 評価基準

- 同じクエリに対して取得されるチャンクの類似度スコアの比較
- RAG応答の正確性と詳細度の比較
- 埋め込み生成時間の測定

## 実験後の分析

実験実施後、以下を記録:

1. **パフォーマンス指標**
   - RAG平均応答長
   - Non-RAG平均応答長
   - チャンク検索の類似度スコア
   - 埋め込み生成時間

2. **定性的評価**
   - 応答の正確性 (1-5)
   - 応答の完全性 (1-5)
   - 文脈との整合性 (1-5)

3. **主要な発見事項**
   - MoE埋め込みによる検索品質の変化
   - exp_001との具体的な違い
   - 計算コストとパフォーマンスのトレードオフ

## 次のステップ

実験完了後:

1. exp_001の結果と詳細に比較
2. exp_004を実行（LM Studio + MoE埋め込み）
3. 4実験の総合比較レポートを作成
4. 埋め込みモデルの選択に関する推奨事項をまとめる
