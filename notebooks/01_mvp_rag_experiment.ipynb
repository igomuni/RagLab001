{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momotaro RAG Experiment - MVP\n",
    "\n",
    "このノートブックでは、桃太郎の物語に関するRAG（Retrieval-Augmented Generation）システムの最小機能を実装します。\n",
    "\n",
    "## 目標\n",
    "- 日本語の桃太郎テキストを読み込む\n",
    "- テキストをチャンクに分割\n",
    "- FAISSベクトルインデックスを作成\n",
    "- テスト質問に対してRAG有無での回答を比較\n",
    "\n",
    "## 必要な準備\n",
    "1. `data/raw/japanese/` に桃太郎のテキストファイルを3-5個配置\n",
    "2. Ollamaが起動していることを確認\n",
    "3. Phi3モデルがインストール済み (`ollama pull phi3`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップとインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Vector store\n",
    "import faiss\n",
    "\n",
    "# LLM\n",
    "import ollama\n",
    "\n",
    "# Encoding detection\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "OLLAMA_MODEL = os.getenv('OLLAMA_MODEL_NAME', 'phi3')\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL_NAME', 'nomic-embed-text')\n",
    "CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', 500))\n",
    "CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', 50))\n",
    "TOP_K = int(os.getenv('TOP_K_RESULTS', 3))\n",
    "VECTOR_DIM = int(os.getenv('VECTOR_DIMENSION', 768))\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_RAW_PATH = PROJECT_ROOT / \"data\" / \"raw\" / \"japanese\"\n",
    "DATA_VECTORS_PATH = PROJECT_ROOT / \"data\" / \"vectors\"\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\" / \"comparisons\"\n",
    "\n",
    "# Create paths if they don't exist\n",
    "DATA_VECTORS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Setup complete\")\n",
    "print(f\"  - Ollama Model: {OLLAMA_MODEL}\")\n",
    "print(f\"  - Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"  - Data path: {DATA_RAW_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データロードと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    ファイルのエンコーディングを自動検出\n",
    "    \"\"\"\n",
    "    result = from_path(file_path).best()\n",
    "    return result.encoding if result else 'utf-8'\n",
    "\n",
    "def load_text_file(file_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    テキストファイルを読み込み、メタデータとともに返す\n",
    "    \"\"\"\n",
    "    encoding = detect_encoding(file_path)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        return {\n",
    "            'source': file_path.name,\n",
    "            'path': str(file_path),\n",
    "            'language': 'ja',\n",
    "            'encoding': encoding,\n",
    "            'content': content,\n",
    "            'char_count': len(content)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    テキストの前処理\n",
    "    - 余分な空白を削除\n",
    "    - 改行を正規化\n",
    "    \"\"\"\n",
    "    # 連続する空白を1つに\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    \n",
    "    # 3つ以上の連続する改行を2つに\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # 行頭・行末の空白を削除\n",
    "    text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# データの読み込み\n",
    "print(\"Loading Japanese Momotaro texts...\")\n",
    "text_files = list(DATA_RAW_PATH.glob('*.txt'))\n",
    "\n",
    "if not text_files:\n",
    "    print(f\"⚠ No text files found in {DATA_RAW_PATH}\")\n",
    "    print(\"Please add Momotaro text files to data/raw/japanese/\")\n",
    "else:\n",
    "    documents = []\n",
    "    for file_path in text_files:\n",
    "        doc = load_text_file(file_path)\n",
    "        if doc:\n",
    "            doc['content'] = preprocess_text(doc['content'])\n",
    "            documents.append(doc)\n",
    "            print(f\"  ✓ Loaded: {doc['source']} ({doc['char_count']:,} chars, {doc['encoding']})\")\n",
    "    \n",
    "    print(f\"\\n✓ Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. テキストのチャンク化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_paragraphs(text: str, max_chars: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"\n",
    "    段落ベースでテキストをチャンク化\n",
    "    段落が長すぎる場合は文単位で分割\n",
    "    \"\"\"\n",
    "    # 段落で分割（空行で区切られた部分）\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # 段落が最大サイズを超える場合\n",
    "        if len(para) > max_chars:\n",
    "            # 現在のチャンクを保存\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            \n",
    "            # 長い段落を文単位で分割\n",
    "            sentences = re.split(r'([。！？])', para)\n",
    "            sentences = [''.join(sentences[i:i+2]) for i in range(0, len(sentences), 2)]\n",
    "            \n",
    "            for sent in sentences:\n",
    "                if len(current_chunk) + len(sent) <= max_chars:\n",
    "                    current_chunk += sent\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sent\n",
    "        else:\n",
    "            # 段落を追加できるか確認\n",
    "            if len(current_chunk) + len(para) + 2 <= max_chars:  # +2 for \\n\\n\n",
    "                if current_chunk:\n",
    "                    current_chunk += \"\\n\\n\" + para\n",
    "                else:\n",
    "                    current_chunk = para\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = para\n",
    "    \n",
    "    # 最後のチャンクを追加\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_chunks_with_metadata(documents: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    全ドキュメントをチャンク化し、メタデータを付加\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = chunk_by_paragraphs(doc['content'], CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        \n",
    "        for idx, chunk_text in enumerate(chunks):\n",
    "            chunk = {\n",
    "                'chunk_id': f\"{doc['source']}_{idx:03d}\",\n",
    "                'text': chunk_text,\n",
    "                'source': doc['source'],\n",
    "                'language': doc['language'],\n",
    "                'position': idx,\n",
    "                'char_count': len(chunk_text)\n",
    "            }\n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# チャンク作成\n",
    "print(\"Creating chunks...\")\n",
    "chunks = create_chunks_with_metadata(documents)\n",
    "print(f\"✓ Created {len(chunks)} chunks\")\n",
    "\n",
    "# チャンクのサンプル表示\n",
    "if chunks:\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"\\n--- Chunk {i+1}: {chunk['chunk_id']} ---\")\n",
    "        print(f\"Source: {chunk['source']}\")\n",
    "        print(f\"Length: {chunk['char_count']} chars\")\n",
    "        print(f\"Text preview: {chunk['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. エンベディング生成とFAISSインデックス構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str, model: str = EMBEDDING_MODEL) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ollamaを使ってテキストのエンベディングを生成\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ollama.embeddings(model=model, prompt=text)\n",
    "        return np.array(response['embedding'], dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_embeddings_batch(texts: List[str], model: str = EMBEDDING_MODEL) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    複数テキストのエンベディングを生成\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Processing {i+1}/{len(texts)}...\")\n",
    "        \n",
    "        emb = generate_embedding(text, model)\n",
    "        if emb is not None:\n",
    "            embeddings.append(emb)\n",
    "        else:\n",
    "            # エラーの場合はゼロベクトル\n",
    "            embeddings.append(np.zeros(VECTOR_DIM, dtype=np.float32))\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n",
    "    \"\"\"\n",
    "    FAISSインデックスを作成\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2距離を使用\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# エンベディング生成\n",
    "print(\"Generating embeddings...\")\n",
    "print(\"This may take a few minutes depending on the number of chunks.\")\n",
    "\n",
    "chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "embeddings = generate_embeddings_batch(chunk_texts)\n",
    "\n",
    "print(f\"✓ Generated {len(embeddings)} embeddings\")\n",
    "print(f\"  Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "# FAISSインデックス作成\n",
    "print(\"\\nCreating FAISS index...\")\n",
    "index = create_faiss_index(embeddings)\n",
    "print(f\"✓ FAISS index created with {index.ntotal} vectors\")\n",
    "\n",
    "# インデックスとメタデータを保存\n",
    "index_path = DATA_VECTORS_PATH / \"index.faiss\"\n",
    "metadata_path = DATA_VECTORS_PATH / \"metadata.json\"\n",
    "\n",
    "faiss.write_index(index, str(index_path))\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Saved index to {index_path}\")\n",
    "print(f\"✓ Saved metadata to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 検索機能の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query: str, index: faiss.Index, chunks: List[Dict], k: int = TOP_K) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    クエリに類似したチャンクを検索\n",
    "    \"\"\"\n",
    "    # クエリのエンベディング生成\n",
    "    query_embedding = generate_embedding(query)\n",
    "    if query_embedding is None:\n",
    "        return []\n",
    "    \n",
    "    # 検索実行\n",
    "    query_embedding = query_embedding.reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # 結果を整形\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx < len(chunks):\n",
    "            result = chunks[idx].copy()\n",
    "            result['distance'] = float(dist)\n",
    "            result['similarity'] = 1 / (1 + dist)  # 類似度スコア\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 検索のテスト\n",
    "print(\"Testing retrieval...\")\n",
    "test_query = \"桃太郎はどこから生まれましたか\"\n",
    "results = search_similar_chunks(test_query, index, chunks, k=3)\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"  Source: {result['source']}\")\n",
    "    print(f\"  Chunk ID: {result['chunk_id']}\")\n",
    "    print(f\"  Similarity: {result['similarity']:.4f}\")\n",
    "    print(f\"  Text: {result['text'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLM統合（Ollama + Phi3）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, model: str = OLLAMA_MODEL, temperature: float = 0.7, max_tokens: int = 500) -> str:\n",
    "    \"\"\"\n",
    "    Ollamaを使って応答を生成\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                'temperature': temperature,\n",
    "                'num_predict': max_tokens\n",
    "            }\n",
    "        )\n",
    "        return response['response']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def build_rag_prompt(query: str, context_chunks: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    RAG用のプロンプトを構築\n",
    "    \"\"\"\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        f\"[参考{i+1}] {chunk['text']}\"\n",
    "        for i, chunk in enumerate(context_chunks)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"以下の参考情報を使って質問に答えてください。\n",
    "\n",
    "【参考情報】\n",
    "{context_text}\n",
    "\n",
    "【質問】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def build_no_rag_prompt(query: str) -> str:\n",
    "    \"\"\"\n",
    "    RAG無しのプロンプトを構築\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"以下の質問に答えてください。\n",
    "\n",
    "【質問】\n",
    "{query}\n",
    "\n",
    "【回答】\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# LLMのテスト\n",
    "print(\"Testing LLM generation...\\n\")\n",
    "\n",
    "test_query = \"桃太郎はどこから生まれましたか\"\n",
    "\n",
    "# RAGありの応答\n",
    "print(\"=== RAG Response ===\")\n",
    "context = search_similar_chunks(test_query, index, chunks, k=3)\n",
    "rag_prompt = build_rag_prompt(test_query, context)\n",
    "rag_response = generate_response(rag_prompt)\n",
    "print(rag_response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# RAG無しの応答\n",
    "print(\"=== Non-RAG Response ===\")\n",
    "no_rag_prompt = build_no_rag_prompt(test_query)\n",
    "no_rag_response = generate_response(no_rag_prompt)\n",
    "print(no_rag_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 評価実験：複数の質問で比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト質問セット\n",
    "test_questions = [\n",
    "    {\n",
    "        'id': 'q1',\n",
    "        'category': 'plot',\n",
    "        'question': '桃太郎はどこから生まれましたか？'\n",
    "    },\n",
    "    {\n",
    "        'id': 'q2',\n",
    "        'category': 'plot',\n",
    "        'question': '桃太郎は誰と一緒に鬼ヶ島へ行きましたか？'\n",
    "    },\n",
    "    {\n",
    "        'id': 'q3',\n",
    "        'category': 'detail',\n",
    "        'question': '桃太郎は動物たちに何をあげましたか？'\n",
    "    },\n",
    "    {\n",
    "        'id': 'q4',\n",
    "        'category': 'plot',\n",
    "        'question': '桃太郎は鬼ヶ島で何をしましたか？'\n",
    "    },\n",
    "    {\n",
    "        'id': 'q5',\n",
    "        'category': 'theme',\n",
    "        'question': '桃太郎の物語が教える教訓は何ですか？'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Running evaluation with {len(test_questions)} questions...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def run_comparison_experiment(questions: List[Dict], index: faiss.Index, chunks: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    全ての質問に対してRAG有無の比較実験を実行\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question {i}/{len(questions)}: {q['question']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # RAGありの応答\n",
    "        print(\"\\nGenerating RAG response...\")\n",
    "        context = search_similar_chunks(q['question'], index, chunks, k=TOP_K)\n",
    "        rag_prompt = build_rag_prompt(q['question'], context)\n",
    "        rag_response = generate_response(rag_prompt)\n",
    "        \n",
    "        print(\"RAG Response:\")\n",
    "        print(rag_response)\n",
    "        \n",
    "        # RAG無しの応答\n",
    "        print(\"\\nGenerating Non-RAG response...\")\n",
    "        no_rag_prompt = build_no_rag_prompt(q['question'])\n",
    "        no_rag_response = generate_response(no_rag_prompt)\n",
    "        \n",
    "        print(\"Non-RAG Response:\")\n",
    "        print(no_rag_response)\n",
    "        \n",
    "        # 結果を保存\n",
    "        result = {\n",
    "            'question_id': q['id'],\n",
    "            'category': q['category'],\n",
    "            'question': q['question'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'rag': {\n",
    "                'response': rag_response,\n",
    "                'context_chunks': [{\n",
    "                    'chunk_id': c['chunk_id'],\n",
    "                    'source': c['source'],\n",
    "                    'text': c['text'],\n",
    "                    'similarity': c['similarity']\n",
    "                } for c in context],\n",
    "                'response_length': len(rag_response)\n",
    "            },\n",
    "            'no_rag': {\n",
    "                'response': no_rag_response,\n",
    "                'response_length': len(no_rag_response)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 実験実行\n",
    "experiment_results = run_comparison_experiment(test_questions, index, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 結果の保存と分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をJSONファイルに保存\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = RESULTS_PATH / f\"experiment_results_{timestamp}.json\"\n",
    "\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(experiment_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の統計分析\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "rag_lengths = [r['rag']['response_length'] for r in experiment_results]\n",
    "no_rag_lengths = [r['no_rag']['response_length'] for r in experiment_results]\n",
    "\n",
    "print(f\"Total questions: {len(experiment_results)}\")\n",
    "print(f\"\\nResponse lengths:\")\n",
    "print(f\"  RAG average: {np.mean(rag_lengths):.1f} chars\")\n",
    "print(f\"  Non-RAG average: {np.mean(no_rag_lengths):.1f} chars\")\n",
    "print(f\"\\nRAG used context from:\")\n",
    "\n",
    "# 使用されたソースの集計\n",
    "sources_used = {}\n",
    "for r in experiment_results:\n",
    "    for chunk in r['rag']['context_chunks']:\n",
    "        source = chunk['source']\n",
    "        sources_used[source] = sources_used.get(source, 0) + 1\n",
    "\n",
    "for source, count in sorted(sources_used.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {source}: {count} times\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nManual evaluation template:\")\n",
    "print(\"For each question, rate both responses on:\")\n",
    "print(\"  1. Accuracy (1-5): Is the answer factually correct?\")\n",
    "print(\"  2. Completeness (1-5): Does it cover all aspects?\")\n",
    "print(\"  3. Coherence (1-5): Is it well-structured?\")\n",
    "print(\"  4. Relevance (1-5): Does it answer the question?\")\n",
    "print(\"\\nResults are saved in JSON format for manual annotation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 結果の可視化（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 日本語フォントの設定\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Hiragino Sans', 'Yu Gothic', 'Meiryo']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 応答長の比較\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "question_ids = [r['question_id'] for r in experiment_results]\n",
    "x = np.arange(len(question_ids))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, rag_lengths, width, label='RAG', alpha=0.8)\n",
    "ax.bar(x + width/2, no_rag_lengths, width, label='Non-RAG', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Question ID')\n",
    "ax.set_ylabel('Response Length (characters)')\n",
    "ax.set_title('Response Length Comparison: RAG vs Non-RAG')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(question_ids)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 次のステップ\n",
    "\n",
    "このMVPが成功したら、以下の拡張を検討:\n",
    "\n",
    "1. **多言語対応**: 英語・中国語のテキストを追加\n",
    "2. **評価の自動化**: 自動評価メトリクスの実装\n",
    "3. **チャンク戦略の最適化**: 異なるチャンクサイズやオーバーラップの実験\n",
    "4. **リランキング**: 検索結果の再順位付け\n",
    "5. **モジュール化**: コードをsrc/配下のモジュールに分割\n",
    "6. **スクリプト化**: scripts/build_index.py と scripts/run_experiment.py の作成\n",
    "\n",
    "---\n",
    "\n",
    "**実験完了！**  \n",
    "結果を確認して、RAGの効果を評価してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
